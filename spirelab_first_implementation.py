# -*- coding: utf-8 -*-
"""spirelab_first implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tyqeAnQOmaaEGcWMAGGiGVIm3LGNsLPO
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install miniaudio
!pip install lidbox==v1.0.0-rc

# Commented out IPython magic to ensure Python compatibility.
# Automatically reload imported modules that are changed outside this notebook
# %load_ext autoreload
# %autoreload 2

# More pixels in figures
import matplotlib.pyplot as plt
# %matplotlib inline
plt.rcParams["figure.dpi"] = 200

# Init PRNG with fixed seed for reproducibility
import numpy as np
np_rng = np.random.default_rng(1)

import tensorflow as tf
tf.random.set_seed(np_rng.integers(0, tf.int64.max))

import urllib.parse
from IPython.display import display, Markdown


languages = """
    et
    mn
    ta
    tr
""".split()

languages = sorted(l.strip() for l in languages)

display(Markdown("### Languages"))
display(Markdown('\n'.join("* `{}`".format(l) for l in languages)))

bcp47_validator_url = 'https://schneegans.de/lv/?tags='
display(Markdown("See [this tool]({}) for a description of the BCP-47 language codes."
                 .format(bcp47_validator_url + urllib.parse.quote('\n'.join(languages)))))

import os


workdir = "/content/drive/MyDrive/SPIRE_Intern_Files/result"
datadir = "/content/drive/MyDrive/SPIRE_Intern_Files/Data"

print("work dir:", workdir)
print("data source dir:", datadir)

os.makedirs(workdir, exist_ok=True)
assert os.path.isdir(datadir), datadir + " does not exist"

dirs = sorted((f for f in os.scandir(datadir) if f.is_dir()), key=lambda f: f.name)
print(dirs)

print(datadir)
for d in dirs:
    if d.name in languages:
        print(' ', d.name)
        for f in os.scandir(d):
            print('   ', f.name)

missing_languages = set(languages) - set(d.name for d in dirs)
assert missing_languages == set(), "missing languages: {}".format(missing_languages)

import pandas as pd
from IPython.display import display, Markdown


# Lexicographic order of labels as a fixed index target to label mapping
target2lang = tuple(sorted(languages))
lang2target = {lang: target for target, lang in enumerate(target2lang)}

print("lang2target:", lang2target)
print("target2lang:", target2lang)


def expand_metadata(row):
    """
    Update dataframe row by generating a unique utterance id,
    expanding the absolute path to the mp3 file,
    and adding an integer target for the label.
    """
    row.id = "{:s}_{:s}".format(
        row.path.split(".mp3", 1)[0].split("common_voice_", 1)[1],
        row.split)
    row.path = os.path.join(datadir, row.lang, "clips", row.path)
    row.target = lang2target[row.lang]
    return row


def tsv_to_lang_dataframe(lang, split):
    """
    Given a language and dataset split (train, dev, test),
    load the Common Voice metadata tsv-file from disk into a pandas.DataFrame.
    Preprocess all rows by dropping unneeded columns and adding new metadata.
    """
    df = pd.read_csv(
        os.path.join(datadir, lang, split + ".tsv"),
        sep='\t',
        # We only need these columns from the metadata
        usecols=("client_id", "path", "sentence"))
    # Add language label as column
    df.insert(len(df.columns), "lang", lang)
    # Add split name to every row for easier filtering
    df.insert(len(df.columns), "split", split)
    # Add placeholders for integer targets and utterance ids generated row-wise
    df.insert(len(df.columns), "target", -1)
    df.insert(len(df.columns), "id", "")
    # Create new metadata columns
    df = df.transform(expand_metadata, axis=1)
    return df


split_names = ("train", "dev", "test")

# Concatenate metadata for all 4 languages into a single table for each split
splits = [pd.concat([tsv_to_lang_dataframe(lang, split) for lang in target2lang])
          for split in split_names]

# Concatenate split metadata into a single table, indexed by utterance ids
meta = (pd.concat(splits)
        .set_index("id", drop=True, verify_integrity=True)
        .sort_index())
del splits

for split in split_names:
    display(Markdown("### " + split))
    display(meta[meta["split"]==split])

def assert_splits_disjoint_by_speaker(meta):
    split2spk = {split: set(meta[meta["split"]==split].client_id.to_numpy())
                 for split in split_names}

    for split, spk in split2spk.items():
        print("split {} has {} speakers".format(split, len(spk)))

    print()
    print("asserting all are disjoint")
    assert split2spk["train"] & split2spk["test"] == set(), "train and test, mutual speakers"
    assert split2spk["train"] & split2spk["dev"]  == set(), "train and dev, mutual speakers"
    assert split2spk["dev"]   & split2spk["test"] == set(), "dev and test, mutual speakers"
    print("ok")


assert_splits_disjoint_by_speaker(meta)

import seaborn as sns


sns.set(rc={'figure.figsize': (8, 6)})
ax = sns.countplot(
    x="split",
    order=split_names,
    hue="lang",
    hue_order=target2lang,
    data=meta)
ax.set_title("Total amount of audio samples")
plt.show()

def path_rename(file_path):
  string = '/content/drive/MyDrive'
  new_filepath = string + file_path[2:]
  new_filepath = new_filepath.replace('\\', '/')
  return new_filepath

import miniaudio


# meta["duration"] = np.array([
#     miniaudio.mp3_get_file_info(path).duration for path in meta.path], np.float32)
# meta
meta_file_path = '/content/drive/MyDrive/SPIRE_Intern_Files/meta.csv'
meta = pd.read_csv(meta_file_path)
meta.set_index('id', inplace=True)
meta['path'] = meta['path'].apply(path_rename)
meta

def plot_duration_distribution(data):
    sns.set(rc={'figure.figsize': (8, 6)})
    
    ax = sns.boxplot(x="split",
        order=split_names,
        y="duration",
        hue="lang",
        hue_order=target2lang,
        data=data)
    ax.set_title("Median audio file duration in seconds")
    plt.show()

    ax = sns.barplot(
        x="split",
        order=split_names,
        y="duration",
        hue="lang",
        hue_order=target2lang,
        data=data,
        ci=None,
        estimator=np.sum)
    ax.set_title("Total amount of audio in seconds")
    plt.show()


plot_duration_distribution(meta)

def random_oversampling(meta):
    groupby_lang = meta[["lang", "duration"]].groupby("lang")
    
    total_dur = groupby_lang.sum()
    target_lang = total_dur.idxmax()[0]
    print("target lang:", target_lang)
    print("total durations:")
    display(total_dur)
    
    total_dur_delta = total_dur.loc[target_lang] - total_dur
    print("total duration delta to target lang:")
    display(total_dur_delta)
    
    median_dur = groupby_lang.median()
    print("median durations:")
    display(median_dur)
    
    sample_sizes = (total_dur_delta / median_dur).astype(np.int32)
    print("median duration weighted sample sizes based on total duration differences:")
    display(sample_sizes)
    
    samples = []
    
    for lang in groupby_lang.groups:
        sample_size = sample_sizes.loc[lang][0]
        sample = (meta[meta["lang"]==lang]
                  .sample(n=sample_size, replace=True, random_state=np_rng.bit_generator)
                  .reset_index()
                  .transform(update_sample_id, axis=1))
        samples.append(sample)

    return pd.concat(samples).set_index("id", drop=True, verify_integrity=True)


def update_sample_id(row):
    row["id"] = "{}_copy_{}".format(row["id"], row.name)
    return row

    
# Augment training set metadata
meta = pd.concat([random_oversampling(meta[meta["split"]=="train"]), meta]).sort_index()

assert not meta.isna().any(axis=None), "NaNs in metadata after augmentation"
plot_duration_distribution(meta)
assert_splits_disjoint_by_speaker(meta)
meta

samples = (meta[meta["split"]=="train"]
           .groupby("lang")
           .sample(n=2, random_state=np_rng.bit_generator))
samples

from IPython.display import display, Audio, HTML
import scipy.signal
import miniaudio


def read_mp3(path, resample_rate=16000):
    if isinstance(path, bytes):
        # If path is a tf.string tensor, it will be in bytes
        path = path.decode("utf-8")
        
    f = miniaudio.mp3_read_file_f32(path)
    
    # Downsample to target rate, 16 kHz is commonly used for speech data
    new_len = round(len(f.samples) * float(resample_rate) / f.sample_rate)
    signal = scipy.signal.resample(f.samples, new_len)
    
    # Normalize to [-1, 1]
    signal /= np.abs(signal).max()
    
    return signal, resample_rate


def embed_audio(signal, rate):
    display(Audio(data=signal, rate=rate, embed=True))

    
def plot_signal(data, figsize=(6, 0.5), **kwargs):
    ax = sns.lineplot(data=data, lw=0.1, **kwargs)
    ax.set_axis_off()
    ax.margins(0)
    plt.gcf().set_size_inches(*figsize)
    plt.show()

    
def plot_separator():
    display(HTML(data="<hr style='border: 2px solid'>"))

    
for sentence, lang, clip_path in samples[["sentence", "lang", "path"]].to_numpy():
    signal, rate = read_mp3(clip_path)
    plot_signal(signal)
    print("length: {} sec".format(signal.size / rate))
    print("lang:", lang)
    print("sentence:", sentence)
    embed_audio(signal, rate)
    plot_separator()

from lidbox.features.audio import spectrograms


def plot_spectrogram(S, cmap="viridis", figsize=None, **kwargs):
    if figsize is None:
        figsize = S.shape[0]/50, S.shape[1]/50
    ax = sns.heatmap(S.T, cbar=False, cmap=cmap, **kwargs)
    ax.invert_yaxis()
    ax.set_axis_off()
    ax.margins(0)
    plt.gcf().set_size_inches(*figsize)
    plt.show()

    
sample = samples[["sentence", "lang", "path"]].to_numpy()[0]
sentence, lang, clip_path = sample

signal, rate = read_mp3(clip_path)
plot_signal(signal)

powspec = spectrograms([signal], rate)[0]

plot_spectrogram(powspec.numpy())

from lidbox.features.audio import power_to_db


dbspec = power_to_db([powspec])[0]
plot_spectrogram(dbspec.numpy())

from lidbox.features.audio import linear_to_mel


def logmelspectrograms(signals, rate):
    powspecs = spectrograms(signals, rate)
    melspecs = linear_to_mel(powspecs, rate, num_mel_bins=40)
    return tf.math.log(melspecs + 1e-6)
    

logmelspec = logmelspectrograms([signal], rate)[0]
plot_spectrogram(logmelspec.numpy())

from lidbox.features import cmvn

logmelspec_mv = cmvn([logmelspec])[0]
plot_spectrogram(logmelspec_mv.numpy())

logmelspec_m = cmvn([logmelspec], normalize_variance=False)[0]
plot_spectrogram(logmelspec_m.numpy())

"""MFCC"""

def plot_cepstra(X, figsize=None):
    if not figsize:
        figsize = (X.shape[0]/50, X.shape[1]/20)
    plot_spectrogram(X, cmap="RdBu_r", figsize=figsize)

    
mfcc = tf.signal.mfccs_from_log_mel_spectrograms([logmelspec])[0]
plot_cepstra(mfcc.numpy())

mfcc = mfcc[:,1:21]
plot_cepstra(mfcc.numpy())

mfcc_cmvn = cmvn([mfcc])[0]
plot_cepstra(mfcc_cmvn.numpy())

from lidbox.features.audio import framewise_rms_energy_vad_decisions
import matplotlib.patches as patches


sentence, lang, clip_path = sample
signal, rate = read_mp3(clip_path)

window_ms = tf.constant(10, tf.int32)
window_frame_length = (window_ms * rate) // 1000

# Get binary VAD decisions for each 10 ms window
vad_1 = framewise_rms_energy_vad_decisions(
    signal=signal,
    sample_rate=rate,
    frame_step_ms=window_ms,
    strength=0.1)

# Plot unfiltered signal
sns.set(rc={'figure.figsize': (6, 0.5)})
ax = sns.lineplot(data=signal, lw=0.1, legend=None)
ax.set_axis_off()
ax.margins(0)

# Plot shaded area over samples marked as not speech (VAD == 0)
for x, is_speech in enumerate(vad_1.numpy()):
    if not is_speech:
        rect = patches.Rectangle(
            (x*window_frame_length, -1),
            window_frame_length,
            2,
            linewidth=0,
            color='gray',
            alpha=0.2)
        ax.add_patch(rect)
plt.show()

print("lang:", lang)
print("sentence: '{}'".format(sentence))
embed_audio(signal, rate)

# Partition the signal into 10 ms windows to match the VAD decisions
windows = tf.signal.frame(signal, window_frame_length, window_frame_length)
# Filter signal with VAD decision == 1 (remove gray areas)
filtered_signal = tf.reshape(windows[vad_1], [-1])

plot_signal(filtered_signal)
print("dropped {:d} out of {:d} frames, leaving {:.3f} of the original signal".format(
    signal.shape[0] - filtered_signal.shape[0],
    signal.shape[0],
    filtered_signal.shape[0]/signal.shape[0]))
embed_audio(filtered_signal, rate)

def remove_silence(signal, rate):
    window_ms = tf.constant(10, tf.int32)
    window_frames = (window_ms * rate) // 1000
    
    # Get binary VAD decisions for each 10 ms window
    vad_1 = framewise_rms_energy_vad_decisions(
        signal=signal,
        sample_rate=rate,
        frame_step_ms=window_ms,
        # Do not return VAD = 0 decisions for sequences shorter than 300 ms
        min_non_speech_ms=300,
        strength=0.1)
    
    # Partition the signal into 10 ms windows to match the VAD decisions
    windows = tf.signal.frame(signal, window_frames, window_frames)
    # Filter signal with VAD decision == 1
    return tf.reshape(windows[vad_1], [-1])


sentence, lang, clip_path = sample
signal, rate = read_mp3(clip_path)

filtered_signal = remove_silence(signal, rate)
plot_signal(filtered_signal)

print("dropped {:d} out of {:d} frames, leaving {:.3f} of the original signal".format(
    signal.shape[0] - filtered_signal.shape[0],
    signal.shape[0],
    filtered_signal.shape[0]/signal.shape[0]))

print("lang:", lang)
print("sentence: '{}'".format(sentence))
embed_audio(filtered_signal, rate)

for sentence, lang, clip_path in samples[["sentence", "lang", "path"]].to_numpy():
    signal_before_vad, rate = read_mp3(clip_path)
    signal = remove_silence(signal_before_vad, rate)
    
    logmelspec = logmelspectrograms([signal], rate)[0]
    logmelspec_mvn = cmvn([logmelspec], normalize_variance=False)[0]
    
    mfcc = tf.signal.mfccs_from_log_mel_spectrograms([logmelspec])[0]
    mfcc = mfcc[:,1:21]
    mfcc_cmvn = cmvn([mfcc])[0]
    
    plot_width = logmelspec.shape[0]/50
    plot_signal(signal.numpy(), figsize=(plot_width, .6))
    print("VAD: {} -> {} sec".format(
        signal_before_vad.size / rate,
        signal.numpy().size / rate))
    print("lang:", lang)
    print("sentence:", sentence)
    embed_audio(signal.numpy(), rate)
    
    plot_spectrogram(logmelspec_mvn.numpy(), figsize=(plot_width, 1.2))
    plot_cepstra(mfcc_cmvn.numpy(), figsize=(plot_width, .6))
    
    plot_separator()

type(samples)

def metadata_to_dataset_input(meta):   
    # Create a mapping from column names to all values under the column as tensors
    return {
        "id": tf.constant(meta.index, tf.string),
        "path": tf.constant(meta.path, tf.string),
        "lang": tf.constant(meta.lang, tf.string),
        "target": tf.constant(meta.target, tf.int32),
        "split": tf.constant(meta.split, tf.string),
    }


sample_ds = tf.data.Dataset.from_tensor_slices(metadata_to_dataset_input(samples))
sample_ds

for x in sample_ds.as_numpy_iterator():
    display(x)

def read_mp3_wrapper(x):
    signal, sample_rate = tf.numpy_function(
        # Function
        read_mp3,
        # Argument list
        [x["path"]],
        # Return value types
        [tf.float32, tf.int64])
    return dict(x, signal=signal, sample_rate=tf.cast(sample_rate, tf.int32))


for x in sample_ds.map(read_mp3_wrapper).as_numpy_iterator():
    print("id: {}".format(x["id"].decode("utf-8")))
    print("signal.shape: {}, sample rate: {}".format(x["signal"].shape, x["sample_rate"]))
    print()

def remove_silence_wrapper(x):
    return dict(x, signal=remove_silence(x["signal"], x["sample_rate"]))


def batch_extract_features(x):
    with tf.device("GPU"):
        signals, rates = x["signal"], x["sample_rate"]
        logmelspecs = logmelspectrograms(signals, rates[0])
        logmelspecs_smn = cmvn(logmelspecs, normalize_variance=False)
        mfccs = tf.signal.mfccs_from_log_mel_spectrograms(logmelspecs)
        mfccs = mfccs[...,1:21]
        mfccs_cmvn = cmvn(mfccs)
    return dict(x, logmelspec=logmelspecs_smn, mfcc=mfccs_cmvn)


features_ds = (sample_ds.map(read_mp3_wrapper)
                 .map(remove_silence_wrapper)
                 .batch(1)
                 .map(batch_extract_features)
                 .unbatch())

for x in features_ds.as_numpy_iterator():
    print(x["id"])
    for k in ("signal", "logmelspec", "mfcc"):
        print("{}.shape: {}".format(k, x[k].shape))
    print()

import lidbox.data.steps as ds_steps


cachedir = os.path.join(workdir, "cache")

_ = ds_steps.consume_to_tensorboard(
    # Rename logmelspec as 'input', these will be plotted as images
    ds=features_ds.map(lambda x: dict(x, input=x["logmelspec"])),
    summary_dir=os.path.join(cachedir, "tensorboard", "data", "sample"),
    config={"batch_size": 1, "image_size_multiplier": 4})

import lidbox.data.steps as ds_steps

TF_AUTOTUNE = tf.data.experimental.AUTOTUNE


def signal_is_not_empty(x):
    return tf.size(x["signal"]) > 0
    

def pipeline_from_metadata(data, shuffle=False):
    if shuffle:
        # Shuffle metadata to get an even distribution of labels
        data = data.sample(frac=1, random_state=np_rng.bit_generator)
    ds = (
        # Initialize dataset from metadata
        tf.data.Dataset.from_tensor_slices(metadata_to_dataset_input(data))
        # Read mp3 files from disk in parallel
        .map(read_mp3_wrapper, num_parallel_calls=TF_AUTOTUNE)
        # Apply RMS VAD to drop silence from all signals
        .map(remove_silence_wrapper, num_parallel_calls=TF_AUTOTUNE)
        # Drop signals that VAD removed completely
        .filter(signal_is_not_empty)
        # Extract features in parallel
        .batch(1)
        .map(batch_extract_features, num_parallel_calls=TF_AUTOTUNE)
        .unbatch()
    )
    return ds


# Mapping from dataset split names to tf.data.Dataset objects
split2ds = {
    split: pipeline_from_metadata(meta[meta["split"]==split], shuffle=split=="train")
    for split in split_names
}

_ = ds_steps.consume(split2ds["train"], log_interval=2000)

os.makedirs(os.path.join(cachedir, "data"))

split2ds["train"] = split2ds["train"].cache(os.path.join(cachedir, "data", "train"))
_ = ds_steps.consume(split2ds["train"], log_interval=2000)

_ = ds_steps.consume(split2ds["train"], log_interval=2000)

for split, ds in split2ds.items():
    _ = ds_steps.consume_to_tensorboard(
            ds.map(lambda x: dict(x, input=x["logmelspec"])),
            os.path.join(cachedir, "tensorboard", "data", split),
            {"batch_size": 1,
             "image_size_multiplier": 2,
             "num_batches": 100},
            exist_ok=True)

"""Traning a supevised neural network language classifier"""

model_input_type = "logmelspec"

def as_model_input(x):
    return x[model_input_type], x["target"]


train_ds_demo = list(split2ds["train"]
                     .map(as_model_input)
                     .shuffle(100)
                     .take(6)
                     .as_numpy_iterator())

for input, target in train_ds_demo:
    print(input.shape, target2lang[target])
    if model_input_type == "mfcc":
        plot_cepstra(input)
    else:
        plot_spectrogram(input)
    plot_separator()

def assert_finite(x, y):
    tf.debugging.assert_all_finite(x, "non-finite input")
    tf.debugging.assert_non_negative(y, "negative target")
    return x, y

_ = ds_steps.consume(split2ds["train"].map(as_model_input).map(assert_finite), log_interval=5000)

x_min = split2ds["train"].map(as_model_input).reduce(
    tf.float32.max,
    lambda acc, elem: tf.math.minimum(acc, tf.math.reduce_min(elem[0])))

x_max = split2ds["train"].map(as_model_input).reduce(
    tf.float32.min,
    lambda acc, elem: tf.math.maximum(acc, tf.math.reduce_max(elem[0])))

print("input tensor global minimum: {}, maximum: {}".format(x_min.numpy(), x_max.numpy()))

import lidbox.models.xvector as xvector


def create_model(num_freq_bins, num_labels):
    model = xvector.create([None, num_freq_bins], num_labels, channel_dropout_rate=0.8)
    model.compile(
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5))
    return model


model = create_model(
    num_freq_bins=20 if model_input_type == "mfcc" else 40,
    num_labels=len(target2lang))
model.summary()

channel_dropout = tf.keras.layers.SpatialDropout1D(model.get_layer("channel_dropout").rate)

for input, target in train_ds_demo:
    print(input.shape, target2lang[target])
    input = channel_dropout(tf.expand_dims(input, 0), training=True)[0].numpy()
    if model_input_type == "mfcc":
        plot_cepstra(input)
    else:
        plot_spectrogram(input)
    plot_separator()

"""Training the classifier"""

callbacks = [
    # Write scalar metrics and network weights to TensorBoard
    tf.keras.callbacks.TensorBoard(
        log_dir=os.path.join(cachedir, "tensorboard", model.name),
        update_freq="epoch",
        write_images=True,
        profile_batch=0,
    ),
    # Stop training if validation loss has not improved from the global minimum in 10 epochs
    tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
    ),
    # Write model weights to cache everytime we get a new global minimum loss value
    tf.keras.callbacks.ModelCheckpoint(
        os.path.join(cachedir, "model", model.name),
        monitor='val_loss',
        save_weights_only=True,
        save_best_only=True,
        verbose=1,
    ),
]

train_ds = split2ds["train"].map(as_model_input).shuffle(1000)
dev_ds = split2ds["dev"].cache(os.path.join(cachedir, "data", "dev")).map(as_model_input)

history = model.fit(
    train_ds.batch(1),
    validation_data=dev_ds.batch(1),
    callbacks=callbacks,
    verbose=2,
    epochs=20)

from lidbox.util import predict_with_model


test_ds = split2ds["test"].map(lambda x: dict(x, input=x["logmelspec"])).batch(1)

_ = model.load_weights(os.path.join(cachedir, "model", model.name))
utt2pred = predict_with_model(model, test_ds)

test_meta = meta[meta["split"]=="test"]
assert not test_meta.join(utt2pred).isna().any(axis=None), "missing predictions"
test_meta = test_meta.join(utt2pred)
test_meta

"""Average detection cost """

from lidbox.util import classification_report
from lidbox.visualize import draw_confusion_matrix


true_sparse = test_meta.target.to_numpy(np.int32)

pred_dense = np.stack(test_meta.prediction)
pred_sparse = pred_dense.argmax(axis=1).astype(np.int32)

report = classification_report(true_sparse, pred_dense, lang2target)

for m in ("avg_detection_cost", "avg_equal_error_rate", "accuracy"):
    print("{}: {:.3f}".format(m, report[m]))
    
lang_metrics = pd.DataFrame.from_dict({k: v for k, v in report.items() if k in lang2target})
lang_metrics["mean"] = lang_metrics.mean(axis=1)
display(lang_metrics.T)

fig, ax = draw_confusion_matrix(report["confusion_matrix"], lang2target)