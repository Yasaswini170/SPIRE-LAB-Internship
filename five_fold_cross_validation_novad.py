# -*- coding: utf-8 -*-
"""Five Fold cross validation_noVAD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cgT4tbgjt5hZNQd7JAQu1JzXb2etBewX
"""

from google.colab import drive  
drive.mount('/content/drive')

!pip install lidbox==v1.0.0-rc

import pandas as pd
import numpy as np
np_rng = np.random.default_rng(1)

from collections import defaultdict
import os
from sklearn import preprocessing

import tensorflow as tf
tf.random.set_seed(np_rng.integers(0, tf.int64.max))

import scipy.io as sio


import lidbox.models.xvector as xvector
from lidbox.util import predict_with_model
from lidbox.util import classification_report
from lidbox.util import draw_confusion_matrix

def create_fold(data_path, filepath, mfcc_dirname):
    datapath = data_path
    fold_data = pd.read_csv(datapath + filepath)
    spa_list = fold_data["SPA no."].tolist()
    mfcc_dir = datapath + '/' + mfcc_dirname
    # vad_dir = datapath + '/' + vad_dirname
    mfcc_files_dict = defaultdict(list)
    # vad_files_dict  = defaultdict(list)
    useless1 = "(1)"
    useless2 = "(2)"
    for spa_num in spa_list:
        for filename in os.listdir(mfcc_dir):
            if (not ((useless1 in filename) or (useless2 in filename))):
              
              if str(spa_num) in filename:
                  mfcc_file_path = mfcc_dir + '/' + filename
                  # vad_file_path  = vad_dir  + '/' + filename
                  mfcc_files_dict[spa_num].append(mfcc_file_path)
                  # vad_files_dict[spa_num].append(vad_file_path)
        
        # for file in os.listdir(vad_dir):
        #     if str(spa_num) in file:
        #         file_path = vad_dir + '/' + file
        #         vad_files_dict[spa_num].append(file_path)
    
    data_ls = []
    for spa_num, file_list in mfcc_files_dict.items():
        for ind, file in enumerate(file_list):
            # vad_file = vad_files_dict[spa_num][ind]
            file_split = file.split('/')
            spa_target = fold_data[fold_data['SPA no.'] == spa_num]['Language of Assessment'].values[0]
            data_ls.append([file_split[-1][:-4], spa_num, file,  spa_target])
    final_df = pd.DataFrame(data_ls, columns=['id', 'SPA no.', 'mfcc_file_path','target_language'])
    final_df['SPA no.'] = final_df['SPA no.'].astype(int)
    final_df = final_df.set_index('id')
    
    return final_df

datapath = r'/content/drive/MyDrive/yasaswini_data'

fold1 = create_fold(datapath,"/ALS_patient_meta_data_fold_1.csv","mfcc_SPON")
fold2 = create_fold(datapath,"/ALS_patient_meta_data_fold_2.csv","mfcc_SPON")
fold3 = create_fold(datapath,"/ALS_patient_meta_data_fold_3.csv","mfcc_SPON")
fold4 = create_fold(datapath,"/ALS_patient_meta_data_fold_4.csv","mfcc_SPON")
fold5 = create_fold(datapath,"/ALS_patient_meta_data_fold_5.csv","mfcc_SPON")

def metadata_to_dataset_input(meta):   
    # Create a mapping from column names to all values under the column as tensors
    return {
        "id": tf.constant(meta.index, tf.string),
        "SPA no.": tf.constant(meta['SPA no.'], tf.string),
        "mfcc_path": tf.constant(meta.mfcc_file_path, tf.string),
        # "vad_path": tf.constant(meta.vad_file_path, tf.string),
        "lang": tf.constant(meta.target_language, tf.string),
        "target": tf.constant(meta.target, tf.int32),
        "split": tf.constant(meta.split, tf.string)
    }

def pipeline_from_metadata(data, shuffle=False):
    if shuffle:
        # Shuffle metadata to get an even distribution of labels
        data = data.sample(frac=1, random_state=np_rng.bit_generator)
    ds = (
        # Initialize dataset from metadata
        tf.data.Dataset.from_tensor_slices(metadata_to_dataset_input(data))
    )
    return ds

def load_mat_file(mfcc_file_path):
    mfcc_dict = sio.loadmat(mfcc_file_path)
    # vad_dict  = sio.loadmat(vad_file_path)
    mfcc_data = mfcc_dict['data'].T
    # vad_data  = vad_dict['data']

    # removing noise
    # non_zero_ind, __ = np.nonzero(vad_data)
    # mfcc_data_filtered = mfcc_data[non_zero_ind]
    return mfcc_data


def as_model_input(x):
    data = tf.numpy_function(load_mat_file, [x['mfcc_path']], [tf.double])
    return data[0], x["target"]


def create_model(num_freq_bins, num_labels):
    model = xvector.create([None, num_freq_bins], num_labels, channel_dropout_rate=0.8)
    model.compile(
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5))
    return model

foldseq = [[1, 2, 3, 4, 5], [2, 3, 4, 5, 1], [3, 4, 5, 1, 2], [4, 5, 1, 2, 3], [5, 1, 2, 3, 4]]
fold_df_dict = {'fold_1':fold1,'fold_2':fold2,'fold_3':fold3,'fold_4':fold4,'fold_5':fold5}
fold_accuracies = []
for ii in range(0,5):
    print('------------------------- Fold: ' + str(ii+1) + ' ------------------------------')
                
    partition = {}
    folds = foldseq[ii]
    
    train_df = pd.concat([fold_df_dict[f'fold_{folds[0]}'],fold_df_dict[f'fold_{folds[1]}'],fold_df_dict[f'fold_{folds[2]}']])
    train_df['split'] = ['train']*len(train_df)
    le = preprocessing.LabelEncoder()
    train_df['target'] = le.fit_transform(train_df['target_language'])
    target2lang = tuple(sorted(le.classes_))
    lang2target = dict(zip(le.classes_, le.transform(le.classes_)))
    partition['train'] = train_df
    

    val_df = fold_df_dict[f'fold_{folds[3]}']
    val_df['split'] = ['validation']*len(val_df)
    val_df['target'] = le.transform(val_df['target_language'])
    partition['validation'] = val_df
    
    test_df = fold_df_dict[f'fold_{folds[4]}']
    test_df['split'] = ['test']*len(test_df)
    test_df['target'] = le.transform(test_df['target_language'])
    partition['test'] = test_df
    
    temp_df=train_df.append(val_df,ignore_index = False)
    final_df=temp_df.append(test_df, ignore_index = False) 

    convert_dict = {'SPA no.': str,
                    'mfcc_file_path': str,
                    'target_language':str,
                    'split' : str,
                    'target' : np.int32
               }
    final_df = final_df.astype(convert_dict)
    # Mapping from dataset split names to tf.data.Dataset objects
    split2ds = {
        split: pipeline_from_metadata(final_df[final_df["split"]==split], shuffle=split=="train")
        for split in partition.keys()
    }

    model = create_model(
                    num_freq_bins=39,
                    num_labels=len(target2lang)
                    )
    model.summary()

    callbacks = [
        # Stop training if validation loss has not improved from the global minimum in 10 epochs
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=20,
        ),
        # Write model weights to cache everytime we get a new global minimum loss value
        tf.keras.callbacks.ModelCheckpoint(
            os.path.join(datapath, "model", model.name),
            monitor='val_loss',
            save_weights_only=True,
            save_best_only=True,
            verbose=1,
        ),
    ]

    train_ds = split2ds["train"].map(as_model_input).shuffle(1000)
    dev_ds = split2ds["validation"].map(as_model_input)

    history = model.fit(
        train_ds.batch(1),
        validation_data=dev_ds.batch(1),
        callbacks=callbacks,
        verbose=2,
        epochs=10)
    
    test_ds = split2ds["test"].map(lambda x: dict(x, input=as_model_input(x)[0])).batch(1)
    utt2pred = predict_with_model(model, test_ds)

    test_df = test_df.join(utt2pred)
    true_sparse = test_df.target.to_numpy(np.int32)

    pred_dense = np.stack(test_df.prediction)
    pred_sparse = pred_dense.argmax(axis=1).astype(np.int32)

    report = classification_report(true_sparse, pred_dense, lang2target)

    print('Test set performance\n')
    for m in ("avg_detection_cost", "avg_equal_error_rate", "accuracy"):
        print("{}: {:.3f}".format(m, report[m]))
        if m == 'accuracy':
          fold_accuracies.append(report[m]
    
    fig, ax = draw_confusion_matrix(report["confusion_matrix"], lang2target)

print(f"The 5-fold CV accuracy: {np.mean(fold_accuracies)}")
print(f"The 5-fold CV stddev: {np.std(fold_accuracies)}")

